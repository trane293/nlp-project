{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Model 1 with Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Anmol Sharma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Open the files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import itertools, sys\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Opening files..\n"
     ]
    }
   ],
   "source": [
    "print('Opening files..', file=sys.stderr)\n",
    "src_set = open('./data/europarl.de', 'rb')\n",
    "des_set = open('./data/europarl.en', 'rb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Split the data into two different sets, and split each sentence into words**\n",
    "2. **Add a NONE character inside every english sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sent = []\n",
    "dest_sent = []\n",
    "\n",
    "for line_des, line_src in zip(des_set, src_set):\n",
    "    # split each sentence into a list of words for easy processing\n",
    "    src_sent.append(line_src.split())\n",
    "    dest_sent.append(line_des.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the words contain many \"\\xc3\\xa9es\"... which are basically the unicode codes for special accented symbols in french. Nothing to worry. \n",
    "\n",
    "Also, the punctuation marks are left as it is as \"words\" which map directly to the punctuation in the destination language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source sentences..\n",
      "[['Ich', 'bitte', 'Sie', ',', 'sich', 'zu', 'einer', 'Schweigeminute', 'zu', 'erheben', '.'], ['(', 'Das', 'Parlament', 'erhebt', 'sich', 'zu', 'einer', 'Schweigeminute', '.', ')'], ['Frau', 'Pr\\xc3\\xa4sidentin', ',', 'zur', 'Gesch\\xc3\\xa4ftsordnung', '.'], ['Wie', 'Sie', 'sicher', 'aus', 'der', 'Presse', 'und', 'dem', 'Fernsehen', 'wissen', ',', 'gab', 'es', 'in', 'Sri', 'Lanka', 'mehrere', 'Bombenexplosionen', 'mit', 'zahlreichen', 'Toten', '.'], ['Zu', 'den', 'Attentatsopfern', ',', 'die', 'es', 'in', 'j\\xc3\\xbcngster', 'Zeit', 'in', 'Sri', 'Lanka', 'zu', 'beklagen', 'gab', ',', 'z\\xc3\\xa4hlt', 'auch', 'Herr', 'Kumar', 'Ponnambalam', ',', 'der', 'dem', 'Europ\\xc3\\xa4ischen', 'Parlament', 'erst', 'vor', 'wenigen', 'Monaten', 'einen', 'Besuch', 'abgestattet', 'hatte', '.']]\n"
     ]
    }
   ],
   "source": [
    "print('Source sentences..', file=sys.stderr)\n",
    "print(src_sent[5:10], file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Destination sentences..\n",
      "[['Please', 'rise', ',', 'then', ',', 'for', 'this', 'minute', \"'\", 's', 'silence', '.'], ['(', 'The', 'House', 'rose', 'and', 'observed', 'a', 'minute', \"'\", 's', 'silence', ')'], ['Madam', 'President', ',', 'on', 'a', 'point', 'of', 'order', '.'], ['You', 'will', 'be', 'aware', 'from', 'the', 'press', 'and', 'television', 'that', 'there', 'have', 'been', 'a', 'number', 'of', 'bomb', 'explosions', 'and', 'killings', 'in', 'Sri', 'Lanka', '.'], ['One', 'of', 'the', 'people', 'assassinated', 'very', 'recently', 'in', 'Sri', 'Lanka', 'was', 'Mr', 'Kumar', 'Ponnambalam', ',', 'who', 'had', 'visited', 'the', 'European', 'Parliament', 'just', 'a', 'few', 'months', 'ago', '.']]\n"
     ]
    }
   ],
   "source": [
    "print('Destination sentences..', file=sys.stderr)\n",
    "print(dest_sent[5:10], file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **We need to find the probability $t_k(f_i|e_j)$ where $f_i$ = source word and $e_j$ = destination word**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find all the unique words in french data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the source list into a chain of iterables, and then convert it to a set to only retain unique elements.\n",
    "# further convert to list for easy processing\n",
    "src_vocab = list(set(itertools.chain.from_iterable(src_sent)))\n",
    "des_vocab = list(set(itertools.chain.from_iterable(dest_sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some unique source words..\n",
      "['L\\xc3\\xa4use', 'Sonderweg', 'verschiedentlich', 'Handschlag', 'Portwein']\n"
     ]
    }
   ],
   "source": [
    "print('Some unique source words..', file=sys.stderr)\n",
    "print(src_vocab[0:5], file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some unique destination words..\n",
      "['gai', 'deferment', 'Pronk', 'woods', 'hanging']\n"
     ]
    }
   ],
   "source": [
    "print('Some unique destination words..', file=sys.stderr)\n",
    "print(des_vocab[0:5], file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the training process.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We cannot initialize the $t_k$ values to uniform due to memory constraints. A better way to do this is to first check if the key exists or not, and if it doesn't, then initialize it to uniform probability. This saves a huge memory and computational overhead of permuting through all $f_i$ and $e_j$ and setting them uniform, many of which will not even appear in the training text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently on training epoch 1..\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "t_k = {}\n",
    "count_comb = {}\n",
    "count_e = {}\n",
    "uni_prob = 1.0 / np.shape(src_vocab)[0]\n",
    "epochs = 5\n",
    "\n",
    "for _i in range(epochs):\n",
    "    print('Currently on training epoch {}..'.format(_i+1), file=sys.stderr)\n",
    "    # iterate over all training examples\n",
    "    for src_sent_eg, dest_sent_eg in zip(src_sent, dest_sent):\n",
    "        for f_i in src_sent_eg:\n",
    "            Z = 0.0\n",
    "            for e_j in dest_sent_eg:\n",
    "                \n",
    "                 # initialize counts on the fl\n",
    "                if (f_i, e_j) not in t_k:\n",
    "#                     print('({}, {}) not in t_k, initializing to uniform!'.format(f_i, e_j))\n",
    "                    t_k[(f_i, e_j)] = 1.0 / uni_prob\n",
    "                \n",
    "                Z += t_k[(f_i, e_j)]\n",
    "            for e_j in dest_sent_eg:\n",
    "                c = t_k[(f_i, e_j)] / Z\n",
    "                \n",
    "                # initialize counts on the fly\n",
    "                if (f_i, e_j) not in count_comb:\n",
    "#                     print('({}, {}) not in count_comb, initializing to 0!'.format(f_i, e_j))\n",
    "                    count_comb[(f_i, e_j)] = 0\n",
    "                \n",
    "                # initialize counts on the fly\n",
    "                if e_j not in count_e:\n",
    "#                     print('({}) not in count_e, initializing to 0!'.format(e_j))\n",
    "                    count_e[e_j] = 0\n",
    "                    \n",
    "                count_comb[(f_i, e_j)] += c\n",
    "                count_e[e_j] += c\n",
    "                \n",
    "    print('Updating t_k counts...', file=sys.stderr)\n",
    "    for f_e_keys in count_comb:\n",
    "        # f_e_keys[0] = f_i, f_e_keys[1] = e_j\n",
    "        t_k[(f_e_keys[0], f_e_keys[1])] = count_comb[f_e_keys] / count_e[f_e_keys[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions using this trained model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Aligning...', file=sys.stderr)\n",
    "print('Source | Destination', file=sys.stderr)\n",
    "for src_sent_eg, dest_sent_eg in zip(src_sent, dest_sent):\n",
    "    i = 0\n",
    "    for f_i in src_sent_eg:\n",
    "        bestp = 0\n",
    "        bestj = 0\n",
    "        j = 0\n",
    "        for e_j in dest_sent_eg:\n",
    "            if t_k[(f_i, e_j)] > bestp:\n",
    "                bestp = t_k[(f_i, e_j)]\n",
    "                bestj = e_j\n",
    "                j += 1\n",
    "        sys.stdout.write('{}-{} '.format(i,j))\n",
    "        i += 1\n",
    "    sys.stdout.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
