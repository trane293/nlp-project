<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h2 id="assignment-1">Assignment 1</h2>
<h2 id="cmpt-825-natural-language-processing">CMPT 825 Natural Language Processing</h2>
<ul>
<li>Student Name: Anmol Sharma</li>
<li>Student ID: asa224</li>
<li>Student Email ID: anmol_sharma@sfu.ca</li>
</ul>
<h2 id="modelling-word-segmentation-problem-in-chinese-text-as-a-sequence-learning-problem-suitable-for-lstms.">Modelling Word Segmentation problem in Chinese Text as a Sequence Learning Problem suitable for LSTMs.</h2>
<p>The following text summarizes the experiments that I performed as I modelled the problem in two different ways, and also discusses the results that were obtained from those implementations. For easy viewing of the code along with its output, an HTML version of the iPython Notebook is added to this directory by the name <code>Seq2Seq_Prediction.html</code>.</p>
<p>The modelling was inspired by a variety of papers, but most notably [1]. For this implementation, I use the 1M Chinese Setences Dataset provided by Prof. Sarkar for this assignment purposes.</p>
<p><em><strong>DISCLAIMER:</strong></em> The ideas for framing the problem statement were inspired by number of readings, many from my previous experience in machine learning, and some from my current literature review. However the code/implementation is entirely my own.</p>
<p><strong>References: [1] Yushi Yao and (2016). Bi-directional LSTM Recurrent Neural Network for Chinese Word Segmentation. CoRR, abs/1602.04874</strong></p>
<h2 id="problem-statements">Problem Statements</h2>
<p>I rephrased the problem of word segmentation in two ways:</p>
<ol style="list-style-type: decimal">
<li><p>[Prob_Def_1] Character-to-Character prediction problem <br> Where given a single character, predict its corresponding label. Labels in this instance are chosen as - B, M, E, and S which stands for Beginning, Middle, End, and Single-letter-word. The problem could also be solved using a simple neural network model, without RNNs, since its basically just predicting a single output given single input without any context. The major downside of this problem was that it did not leverage the power of RNNs where it utilizes the time dimensions to make decision. However it may be argued that neural network has an internal representation that encodes similar information in this case. This problem was however implemented using an LSTM unit based RNN in Python/Keras.</p></li>
<li><p>[Prob_Def_2] Sequence-to-Sequence prediction problem <br> Where given a sequence, predict a sequence of labels. This rephrasing of problem allows me to utilize the time dimension property of RNN where it sees the context of each character while making the decision predicting the corresponding label. This problem was implemented using LSTM unit based RNN in Python/Keras.</p></li>
</ol>
<p><strong>Now I provide an overview of the various data preprocessing steps common to both of these problem definitions that were performed in order bring the dataset close to what I could for training a neural network model.</strong></p>
<h3 id="parsing-and-assigning-labels">Parsing and Assigning Labels</h3>
<p>Given a set of words, we assign the labels to each character seen in the training example. For example:</p>
<p><code>A GOOD MAN ANMOL SHARMA</code> <br> <code>| |||| ||| ||||| ||||||</code> <br> <code>S BMME BME BMMME BMMMME</code></p>
<p>The labels were converted to categorical vector representation, using the following truth table -</p>
<pre><code>      | Integer | Vector
Label | Value   | Representation
--------------------------------
B     | 0       | 1,0,0,0
M     | 1       | 0,1,0,0
E     | 2       | 0,0,1,0
S     | 3       | 0,0,0,1</code></pre>
<p>This parsing produces a list of tuples of the form <code>[(u'\ue12as', 0), (u'\ue4a3', 2)....(u'\u2354', 3)]</code> where the first element of the tuple is a character and the second element is the class label. It looks like this:</p>
<div class="figure">
<img src="ims/sequence.png" alt="Nothing" />
<p class="caption">Nothing</p>
</div>
<p>For [Prob_Def_1] the issue arises where the newline character either have to be 1) ignored, or 2) incorporated in the parsing, probably as a standalone character. After some initial experiments, I found that ignoring newline would lead me to lose information about lines, which the network cannot determine at test time. So the network lead to generating an output text with basically no new lines. After some literature review, I found out that actually parsing newline as a single-letter-character can help the RNN make decisions, since context often changes after each line, and newline character can help RNN to make that decision.</p>
<p>Hence for [Prob_Def_2] we parse newline characters as having label &quot;S&quot;. One interesting implementation related fact is that the newline chracter is encoded as a special symbol in unicode, since I was having issues reading it back from a file. The special symbol is chosen at random,and appears to be a degree sign.</p>
<h3 id="initial-integer-embedding">Initial Integer Embedding</h3>
<p>Machine learning models cannot directly work on categorical data, the issue arises that the data must converted to an integral representation to make it compatible for training. The method that I use for this is simple, and is highlighted here using English language -</p>
<p>Given an English alphabet of unique characters, we find there are 26 unique characters. Hence, for each character, we assign an integer value such that:</p>
<p><code>A B ...... X   Y   Z</code><br> <code>| | ...... |   |   |</code><br> <code>0 1 ...... 23 24 25</code><br></p>
<p>I implement this for chinese characters by finding the number of unique characters seen in the training set, which totalled to about 5920. A pair of dictionaries were generated (one for word-to-int and other for int-to-word) for converting the dataset.</p>
<h3 id="chunking-into-sequences">Chunking into Sequences</h3>
<p>For [Prob_Def_1], we used the following approach:</p>
<ol style="list-style-type: decimal">
<li>Generate each sentence in the training data as a single sequence.</li>
<li>Use a moderate value for fixing the sequence size to, in my case I used <code>maxlen = 200.</code></li>
<li>For each <code>sequence)</code>:</li>
<li>if len(sequence) &lt; maxlen:
<ol style="list-style-type: decimal">
<li>Pad the sequence until <code>len(sequence) == maxlen</code></li>
</ol></li>
<li>else:
<ol style="list-style-type: decimal">
<li>Truncate the sequence from front so that <code>len(sequence) == maxlen</code></li>
</ol></li>
</ol>
<p>This approach was necessary since RNNs expect inputs to have a set size.</p>
<p>However, for [Prob_Def_2] I experimented with constant size sequences, where I create each sequence of <code>maxlen = 13</code> along with parsing the newline character. This value was used after reading the paper [1]</p>
<h2 id="training-data">Training Data</h2>
<p>After the preprocessing is done, the training set is generated with shapes as follows:</p>
<div class="figure">
<img src="ims/data_shape.png" alt="Nothing" />
<p class="caption">Nothing</p>
</div>
<h2 id="defining-rnn-model">Defining RNN Model</h2>
<p>The model that I train for [Prob_Def_1] is as follows:</p>
<pre><code>model = Sequential()
# first argument is the size of the vocabulary, second argument is the size of embedding, third argument is the
# number of features in the text, we only have 1 character.
model.add(Embedding(len(orig_dict), 200, input_length=1))
model.add(Bidirectional(LSTM(10, return_sequences=True)))
model.add(Bidirectional(LSTM(10)))
model.add(Dropout(0.2))
model.add(Dense(4, activation=&#39;softmax&#39;))
model.compile(&#39;adadelta&#39;, &#39;categorical_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
model.summary()</code></pre>
<p>The model that I train for [Prob_Def_2] is as follows: ~<sub>~</sub> model = Sequential() model.add(Embedding(input_dim=len(orig_dict), output_dim=200, input_length=n_timesteps)) # dictionary size, embedding vector size, timesteps model.add(LSTM(300, return_sequences=True)) model.add(LSTM(300, return_sequences=True)) model.add(Dropout(0.2)) model.add(TimeDistributed(Dense(4, activation='softmax'))) model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc']) model.summary() ~<sub>~</sub></p>
<p>The model for the above code has the following output shapes per each layer:</p>
<div class="figure">
<img src="ims/model_seq2seq.png" alt="Nothing" />
<p class="caption">Nothing</p>
</div>
<h3 id="embedding-layer">Embedding Layer</h3>
<p><code>model.add(Embedding(input_dim=len(orig_dict), output_dim=200,</code><br> An embedding layer was used as the input layer to our RNN which learns the most effective emdedding of the words using a set of weights by projecting each character onto a d-dimensional space and finding the most suitable internal representation (embedding) that maximized the goal of RNN (in this case, produce accurate predictions). The embedding maxlen was set to <code>output_dim = 200</code>, which is the same value used in paper [1]. input_dim parameter takes the number of unique characters in the training corpus.</p>
<h3 id="lstm-layer">LSTM Layer</h3>
<p><code>model.add(LSTM(300, return_sequences=True))</code></p>
<p>Long-Short Term Memory Networks is an extension of Recurrent Neural Networks which addresses two main problems plaguing traditional RNNs:</p>
<ol style="list-style-type: decimal">
<li>Vanishing/Exploding gradient problem.</li>
<li>Retaining long-term memory, which is sometimes required for modelling data that has long-term dependencies, like text.</li>
</ol>
<p>LSTM unit generally looks like this:</p>
<div class="figure">
<img src="ims/lstm_unit.png" alt="Nothing" />
<p class="caption">Nothing</p>
</div>
<p>(Figure courtesy - http://colah.github.io/posts/2015-08-Understanding-LSTMs/)</p>
<p>It has usually three gates -</p>
<ol style="list-style-type: decimal">
<li>Input Gate</li>
<li>Output Gate</li>
<li>Forget Gate</li>
</ol>
<p>In my implementation, I use an LSTM layer with 300 LSTM units. Each LSTM layer usually outputs only the predicted sequence, however in my case where I wanted to stack another LSTM on top of it, the layer returns back input sequences along with its predictions which are then passed on to the next LSTM layer.</p>
<h3 id="dropout-layer">Dropout Layer</h3>
<p><code>model.add(Dropout(0.2))</code></p>
<p>This layer turns off the input neurons randomly to address the issue of overtraining. Usually the layer generates a random number for each input neuron (or activation), and if the number is greater than the given probability (here <code>p = 0.2</code>) the neuron is turned off. This allows the network to learn features that are not highly correlated with other features.</p>
<h3 id="time-distributed-dense-layer">Time Distributed Dense Layer</h3>
<p><code>model.add(TimeDistributed(Dense(4, activation='softmax')))</code></p>
<p>The TimeDistributed wrapper in Keras allows the network to apply the Dense layer (which has the same number of neurons as the output classes, which in our case <code>n_output = 4</code>) to each timestamp (<code>n_timesteps=13</code> in our case) with the same weights. This is useful in problems which are inherently Many-to-Many Sequence modelling problem, which our [Prob_Def_2] essentially is.</p>
<h2 id="train-the-model">Train the Model</h2>
<pre><code>model.fit(X_train, y_train, epochs=10,
          batch_size=100, verbose=1, validation_data=[X_test, y_test], callbacks=[mc])</code></pre>
<p>The model was trained using <code>Adadelta</code> optimizer with default values for learning rate (<code>lr = 1.0</code>), rho (<code>rho = 0.95</code>) and epsilon (<code>eps = 1e-8</code>). Adadelta optimizer is designed in a way that makes the initial choise of learning rate less determining of the fact that the network converges to a good optimal or not. It does this by automatically throttling learning rate using the current values of gradients that propagates through the network.</p>
<p>The main database of 1M chinese sentences was divided into two disjoint sets for training and testing. The ratio of training to whole dataset was 80% and test set to whole dataset was 20%.</p>
<p>A single epoch of training gives good results on both training and test set, as shown below:</p>
<div class="figure">
<img src="ims/training.png" alt="Nothing" />
<p class="caption">Nothing</p>
</div>
<h2 id="discussion-and-conclusion">Discussion and Conclusion</h2>
<p>The accuracy of the model goes upto 90% on test set, and around 91% on the training set after about 10 epochs of training. Each epoch takes approximately 800s to complete.</p>
<p>However, the accuracy on test set during this phase is not entirely reflected on the test data that Prof. Sarkar provided. The test set accuracy for the given test input was about 75% for this model. After more in-depth analysis and review of the issues, I concluded the following:</p>
<p>The problem definition with 4 different labels makes it hard to take decision during the test time. This can be explained using a simple example:</p>
<p>Let the test sequence be: <code>ANMOLSHARMA</code> <br></p>
<p>and the predicted sequence be: <code>BEMMEBMMMMS</code></p>
<p>Out of 10 values the network &quot;correctly&quot; predicted 8 values, but incorrectly predicted 2 values. This makes the accuracy of the network 80%. However, the incorrect predictions directly impact the segmentation of the word. The question arises, how do we segment the word?</p>
<p><code>AN MOL SHARM A</code></p>
<p>or</p>
<p><code>AN MOL SHARMA</code></p>
<p>The approach that I applied suffers due to these ambiguities, and explicit rules have to be written during post-processing to handle such cases.</p>
<p><em><strong>However, such rules are subjective. Hence the approach performed poorly than a unigram segmenter. </strong></em></p>
<p>Unfortunately I figured this out AFTER doing all this, but nonetheless it was a great exercise, and I learnt a great deal about RNNs, LSTMs, BLSTM, and how to implement actual NLP solutions.</p>
</body>
</html>
