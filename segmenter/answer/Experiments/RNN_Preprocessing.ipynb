{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:orange'> Chinese Word Segmentation using Bidirectional LSTMs </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='color:green'> Building the Dataset for Training <span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the Wseg_1M Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write a wrapper for the below function to incorporate the 1M chinese word segm dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we basically parse the input file and find all spaces. As soon as we find a space we output the word into a text\n",
    "# file called word_file_1M which has the same format as the count_1w file.\n",
    "\n",
    "with(open('/local-scratch/wseg_simplified_cn.txt', 'rb')) as f:\n",
    "    word_file_1M = open('/local-scratch/word_file_1M', 'wb')\n",
    "    for line in f:\n",
    "        line = unicode(line, 'utf-8')\n",
    "        line = line.replace('\\n', '')\n",
    "        line = line.split(' ')\n",
    "        for word in line:\n",
    "            word_file_1M.write(word.encode('utf-8') + '\\t'.encode('utf-8') + str(0).encode('utf-8') +\\\n",
    "                                       '\\n'.encode('utf-8'))\n",
    "    f.close()\n",
    "word_file_1M.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning labels to characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels:\n",
    "\n",
    "0 - Beginning <br>\n",
    "1 - Middle <br>\n",
    "2 - End <br>\n",
    "3 - Single Character word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we open the count file, get the word and assign labels to each character\n",
    "# cannot use dictionaries, since the same character may appear again and overwrites the value at its place in dict.\n",
    "with(open('/local-scratch/word_file_1M', 'rb')) as f:\n",
    "    label = []\n",
    "    for line in f:\n",
    "        word, count = line.split('\\t')\n",
    "        \n",
    "        # making sure the parsing is going fine\n",
    "        assert int(count) == 0\n",
    "        \n",
    "        word = unicode(word, 'utf-8')\n",
    "        if len(word) == 1:\n",
    "            label.append((word[0], 3))\n",
    "        else:\n",
    "            for i, character in enumerate(word):\n",
    "                if i == 0: # this is the first letter\n",
    "                    label.append((character, 0))\n",
    "                elif i == (len(word) - 1): # this is the last letter\n",
    "                    label.append((character, 2))\n",
    "                else: # this is somewhere in the middle\n",
    "                    label.append((character, 1))\n",
    "                    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building initial integer embeddings for characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [y[0] for y in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return dictionary, reverse_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dict, ret_dict = build_dataset(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_dict[orig_dict[u'\\u6753']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [orig_dict[y[0]] for y in label]\n",
    "y_train = [y[1] for y in label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change y_train into categorical one-hot-vector encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(x_train), 'train sequence shape')\n",
    "print(np.shape(y_train), 'labels shape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train a BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# first argument is the size of the vocabulary, second argument is the size of embedding, third argument is the\n",
    "# number of features in the text, we only have 1 character. \n",
    "model.add(Embedding(len(orig_dict), 200, input_length=1))\n",
    "model.add(Bidirectional(LSTM(10, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(10)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile('adadelta', 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(np.array(x_train), y_train,\n",
    "          batch_size=20,\n",
    "          epochs=1,\n",
    "          validation_data=[x_train, y_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on test set and generating output file suitable for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the input file, open the output file\n",
    "inp_file = open('../data/input', 'rb')\n",
    "out_file = open('../data/output_rnn', 'wb')\n",
    "\n",
    "# read the first line\n",
    "\n",
    "for line in inp_file:\n",
    "    # divide the line into characters\n",
    "    line = unicode(line, 'utf-8')\n",
    "    line = line.replace('\\n','')\n",
    "    print(line)\n",
    "    init_embedding = [orig_dict[char] for char in line]\n",
    "        \n",
    "    # change the character into initial embedding using orig_dict\n",
    "    \n",
    "    # feed into RNN\n",
    "        \n",
    "    pred_labels = model.predict_classes(init_embedding)\n",
    "    \n",
    "    # get the class label\n",
    "    \n",
    "    for num, char in enumerate(line):\n",
    "        if pred_labels[num] >= 2:\n",
    "            out_file.write(char.encode('utf-8') + ' '.encode('utf-8'))\n",
    "        else:\n",
    "            out_file.write(char.encode('utf-8'))\n",
    "    \n",
    "    out_file.write('\\n'.encode('utf-8'))\n",
    "    \n",
    "out_file.close()\n",
    "    # if label == 2 or label == 3, add a trailing space after the character\n",
    "\n",
    "    # output the line into a file\n",
    "    # f = open('myfile', 'w')\n",
    "    # f.write('hi there\\n')  # python will convert \\n to os.linesep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
