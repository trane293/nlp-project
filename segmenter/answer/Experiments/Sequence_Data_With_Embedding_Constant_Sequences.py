
# coding: utf-8

# # <span style='color:orange'> Modelling Chinese Word Segmentation as Sequence to Sequence Prediction Problem </span>

# # Generate data for sequence to sequence modelling

# This returns the structure "data" which contains sentences as individual lists, along with the class label of each character. 

# In[1]:


import numpy as np
import collections
from sklearn.model_selection import train_test_split
import keras
from keras.utils.np_utils import to_categorical
from keras.preprocessing import sequence


# In[2]:


# we open the count file, get the word and assign labels to each character
# cannot use dictionaries, since the same character may appear again and overwrites the value at its place in dict.
def generateTupleListAccToSentences(filename='/local-scratch/asa224/wseg_simplified_cn.txt'):
    """
    This function generates a data list of lists, which contains sequences and corresponding
    labels for each character, according to the sentences in the input file. This function 
    takes the whole training set txt file as input, and generates sequences according to the 
    line, ie. each sequence is a line. 
    
    If you want to use this data as training data for LSTM, you have to pad the sequences 
    since they are not of the same length. 
    """
    # filepath = '/mnt/D82A1A8F2A1A6B30/wseg_simplified_cn.txt'
    with(open(filepath, 'rb')) as f:
        data = [[]]
        count = 0
        for line in f:
            line = unicode(line, 'utf-8')
            line = line.replace('\n', '')
            words = line.split(' ')

            for word in words:
                if len(word) == 1:
                    data[count].append((word[0], 3))
                else:
                    for i, character in enumerate(word):
                        if i == 0: # this is the first letter
                            data[count].append((character, 0))
                        elif i == (len(word) - 1): # this is the last letter
                            data[count].append((character, 2))
                        else: # this is somewhere in the middle
                            data[count].append((character, 1))
            data.append([])
            count += 1

        f.close()
        
        return data
    
def generateTupleList(filename='/local-scratch/asa224/word_file_1M'):
    """
    This function is similar to the above function in the sense that it assigns labels to each
    character in the training set. The function returns a list of tuples, in which each tuple
    contains a single character and its corresponding label. 
    
    Use this function in conjunction with nGramSequenceGenerator(labelledlist, n) to create a
    training set with constant sequence size, which does not require paddings. 
    """
    with(open(filename, 'rb')) as f:
        label = []
        for line in f:
            word, count = line.split('\t')

            # making sure the parsing is going fine
            assert int(count) == 0

            word = unicode(word, 'utf-8')
            if len(word) == 1:
                label.append((word[0], 3))
            else:
                for i, character in enumerate(word):
                    if i == 0: # this is the first letter
                        label.append((character, 0))
                    elif i == (len(word) - 1): # this is the last letter
                        label.append((character, 2))
                    else: # this is somewhere in the middle
                        label.append((character, 1))

        f.close()
        return label
    
def nGramSequenceGenerator(labelledlist, n):
    """
    Takes as input the label list of tuples generated by the code above. 
    The function generates sequence of size "n" from the given list. 
    """
    count = len(labelledlist)/n
    ngrammedlist = []
    for i in range(count):
        ngrammedlist.append( labelledlist[i*n : (i+1)*n])
    return ngrammedlist


# In[3]:


list_of_tuples = generateTupleList(filename='/local-scratch/asa224/word_file_1M')
data = nGramSequenceGenerator(list_of_tuples, n=10)


# In[4]:


data[0:5]


# # Build initial integer embeddings

# In[5]:


all_chars = [data[i][j][0] for i in range(0, len(data)) for j in range(0, len(data[i]))]


# In[6]:


len(all_chars)


# In[7]:


def build_dataset(words):
    count = collections.Counter(words).most_common()
    dictionary = dict()
    for word, _ in count:
        dictionary[word] = len(dictionary)
    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))
    return dictionary, reverse_dictionary


# In[8]:


orig_dict, ret_dict = build_dataset(all_chars)
del all_chars


# In[9]:


ret_dict[orig_dict[u'\u6743']]


# In[10]:


len(orig_dict)


# ## Create sequences suitable for training

# In[11]:


x = [[]]
y = [[]]
for i in range(0, len(data)): # iterate over the whole dataset
    for j in range(0, len(data[i])): # iterate over the current sentence
        x[i].append(orig_dict[data[i][j][0]])
        y[i].append(data[i][j][1])
    x.append([])
    y.append([])


# In[12]:


del data


# In[13]:


X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.20, shuffle=False, random_state=42)


# In[14]:


print('Size of the Dataset\n')
print('X_TRAIN: {}'.format(np.shape(X_train)))
print('Y_TRAIN: {}'.format(np.shape(y_train)))

print('X_TEST: {}'.format(np.shape(X_test)))
print('Y_TEST: {}'.format(np.shape(y_test)))


# ## Convert the labels to categorical. 

# This is what they look like now

# In[15]:


for i in y_train:
    print(i)
    break


# ### Start conversion

# Testing labels <br>
# There are empty lists in the test set, let's remove them. 

# In[16]:


X_train = [t for t in X_train if t != []]
y_train = [t for t in y_train if t != []]

X_test = [t for t in X_test if t != []]
y_test = [t for t in y_test if t != []]


# Training labels

# In[17]:


for i in range(0, len(y_train)):
    y_train[i] = to_categorical(y_train[i], num_classes=4)


# In[18]:


for i in range(0, len(y_test)):
    y_test[i] = to_categorical(y_test[i], num_classes=4)


# This how it looks now

# In[19]:


for i in y_train:
    print(i)
    break


# Firstly, we must update the get_sequence() function to reshape the input and output sequences to be 3-dimensional to meet the expectations of the LSTM. The expected structure has the dimensions [samples, timesteps, features]. The classification problem has 1 sample (e.g. one sequence), a configurable number of timesteps, and one feature per timestep.

# Let's reshape the training and testing data

# ### No need to pad sequences when working on constant size sequences

# In[ ]:


# # maximum length of the sequence
# maxval = len(max(X_train,key=len))


# In[ ]:


# X_train = sequence.pad_sequences(X_train, value=-1, maxlen=100)
# y_train = sequence.pad_sequences(y_train, value=-1, maxlen=100)

# X_test = sequence.pad_sequences(X_test, value=-1, maxlen=100)
# y_test = sequence.pad_sequences(y_test, value=-1, maxlen=100)


# In[ ]:


embedding = True


# In[21]:


# reshape input and output data to be suitable for LSTMs
if embedding == False:
    n_timesteps = 10
    X_train = np.array(X_train).reshape(np.shape(X_train)[0], n_timesteps, 1)
    y_train = np.array(y_train).reshape(np.shape(y_train)[0], n_timesteps, 4)

    X_test = np.array(X_test).reshape(np.shape(X_test)[0], n_timesteps, 1)
    y_test = np.array(y_test).reshape(np.shape(y_test)[0], n_timesteps, 4)
    
else:
    # reshape input and output data to be suitable for LSTMs
    n_timesteps = 10
    X_train = np.array(X_train).reshape(np.shape(X_train)[0], n_timesteps)
    y_train = np.array(y_train).reshape(np.shape(y_train)[0], n_timesteps, 4)

    X_test = np.array(X_test).reshape(np.shape(X_test)[0], n_timesteps)
    y_test = np.array(y_test).reshape(np.shape(y_test)[0], n_timesteps, 4)


# ## IF you use embedding layer, use this code

# In[22]:


X_train.shape


# TODO: Add embedding layer <br>
# TODO: Increase LSTM units <br>
# TODO: Decrease the sequence size <br>
# TODO: Decrease the sequence size from the training set generation itself <br>

# In[31]:


# LSTM for sequence classification in the IMDB dataset
import numpy
from keras.datasets import imdb
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers.embeddings import Embedding
from keras.preprocessing import sequence
# fix random seed for reproducibility
numpy.random.seed(7)
# load the dataset but only keep the top n words, zero the rest
top_words = 5000
(x1, y1), (x2, y2) = imdb.load_data(nb_words=top_words)
# truncate and pad input sequences
max_review_length = 500
x1 = sequence.pad_sequences(x1, maxlen=max_review_length)
x2 = sequence.pad_sequences(x2, maxlen=max_review_length)


# In[32]:


x1.shape


# In[34]:


from keras.models import Sequential
from keras.layers import LSTM, TimeDistributed, Dense, Masking, Embedding, Dropout
# define LSTM
model = Sequential()
model.add(Embedding(input_dim=len(orig_dict), output_dim=200, input_length=10)) # dictionary size, embedding vector size, timesteps
model.add(LSTM(300, return_sequences=True))
model.add(LSTM(300, return_sequences=True))
model.add(Dropout(0.3))
model.add(TimeDistributed(Dense(4, activation='softmax')))
model.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['acc'])
model.summary()


# In[28]:


X_test.shape


# In[35]:


history = model.fit(X_train, y_train, epochs=1, 
          batch_size=100, verbose=1, validation_data=[X_test, y_test])


# In[ ]:


history2 = model.fit(X_train.reshape(np.shape(X_train)[0], 10), y_train, epochs=1, 
          batch_size=100, verbose=1, validation_data=[X_test.reshape(np.shape(X_test)[0], 10), y_test])

